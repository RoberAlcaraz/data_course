{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides various facilities for easily combining together Series or DataFrame with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.\n",
    "\n",
    "* merge() for combining data on common columns or indices\n",
    "* join() for combining data on a key column or an index\n",
    "* concat() for combining DataFrames across rows or columns\n",
    "\n",
    "In addition, pandas also provides utilities to compare two Series or DataFrame and summarize their differences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concat\n",
    "\n",
    "The pandas.concat() function does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (**if any**) on the other axes. Note that I say “if any” because there is only a single possible axis of concatenation for Series.\n",
    "\n",
    "![merge](../images/pd_merging_concat_basic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    },\n",
    "    index=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A4\", \"A5\", \"A6\", \"A7\"],\n",
    "        \"B\": [\"B4\", \"B5\", \"B6\", \"B7\"],\n",
    "        \"C\": [\"C4\", \"C5\", \"C6\", \"C7\"],\n",
    "        \"D\": [\"D4\", \"D5\", \"D6\", \"D7\"],\n",
    "    },\n",
    "    index=[4, 5, 6, 7],\n",
    ")\n",
    "\n",
    "\n",
    "df3 = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"A8\", \"A9\", \"A10\", \"A11\"],\n",
    "        \"B\": [\"B8\", \"B9\", \"B10\", \"B11\"],\n",
    "        \"C\": [\"C8\", \"C9\", \"C10\", \"C11\"],\n",
    "        \"D\": [\"D8\", \"D9\", \"D10\", \"D11\"],\n",
    "    },\n",
    "    index=[8, 9, 10, 11],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1, df2, df3]\n",
    "result = pd.concat(frames)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a list or dict of homogeneously-typed objects and concatenates them with some configurable handling of “what to do with the other axes”\n",
    "\n",
    "Documentation: [concat](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)\n",
    "\n",
    "\n",
    "Suppose we wanted to associate specific keys with each of the pieces of the chopped up DataFrame. We can do this using the keys argument:\n",
    "\n",
    "![concat_keys](../images/pd_merging_concat_keys.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(frames, keys=[\"x\", \"y\", \"z\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we can use indices as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat makes a full copy of the data, and that constantly reusing this function can create a significant performance hit. If you need to use the operation over several datasets, use a list comprehension.\n",
    "\n",
    "```\n",
    "frames = [ process_your_file(f) for f in files ]\n",
    "result = pd.concat(frames)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame(\n",
    "    {\n",
    "        \"B\": [\"B2\", \"B3\", \"B6\", \"B7\"],\n",
    "        \"D\": [\"D2\", \"D3\", \"D6\", \"D7\"],\n",
    "        \"F\": [\"F2\", \"F3\", \"F6\", \"F7\"],\n",
    "    },\n",
    "    index=[2, 3, 6, 7],\n",
    ")\n",
    "\n",
    "\n",
    "result = pd.concat([df1, df4], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df4], axis=1, join=\"inner\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we just wanted to reuse the exact index from the original DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df4], axis=1).reindex(df1.index)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring indexes on the concatenation axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df4], ignore_index=True, sort=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concat_ignore](../images/pd_merging_concat_ignore_index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating mixed dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([\"X0\", \"X1\", \"X2\", \"X3\"], name=\"X\")\n",
    "\n",
    "result = pd.concat([df1, s1], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating  with group keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = pd.Series([0, 1, 2, 3], name=\"foo\")\n",
    "\n",
    "s4 = pd.Series([0, 1, 2, 3])\n",
    "\n",
    "s5 = pd.Series([0, 1, 4, 5])\n",
    "\n",
    "pd.concat([s3, s4, s5], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s3, s4, s5], axis=1, keys=[\"red\", \"blue\", \"yellow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(frames, keys=[\"x\", \"y\", \"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = {\"x\": df1, \"y\": df2, \"z\": df3}\n",
    "\n",
    "result = pd.concat(pieces)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(pieces, keys=[\"z\", \"y\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.index.levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(\n",
    "    pieces, keys=[\"x\", \"y\", \"z\"], levels=[[\"z\", \"y\", \"x\", \"w\"]], names=[\"group_key\"]\n",
    ")\n",
    "result.index.levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending rows to a df\n",
    "\n",
    "You should use ignore_index with this method to instruct DataFrame to discard its index. If you wish to preserve the index, you should construct an appropriately-indexed DataFrame and append or concatenate those objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.Series([\"X0\", \"X1\", \"X2\", \"X3\"], index=[\"A\", \"B\", \"C\", \"D\"])\n",
    "\n",
    "result = pd.concat([df1, s2.to_frame().T], ignore_index=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL.\n",
    "\n",
    "See the [merge cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html#cookbook-merge)\n",
    "\n",
    "pandas provides a single function, merge(), as the entry point for all standard database join operations between DataFrame or named Series objects\n",
    "\n",
    "Disclaimer: The related join() method, uses merge internally for the index-on-index (by default) and column(s)-on-index join. If you are joining on index only, you may wish to use DataFrame.join to save yourself some typing.\n",
    "\n",
    "There are several cases to consider which are very important to understand:\n",
    " * one-to-one joins: for example when joining two DataFrame objects on their indexes (which must contain unique values).\n",
    " * many-to-one joins: for example when joining an index (unique) to one or more columns in a different DataFrame.\n",
    " * many-to-many joins: joining columns on columns.\n",
    " \n",
    "![merge_key](../images/pd_merging_merge_on_key.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame(\n",
    "    {\n",
    "        \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "right = pd.DataFrame(\n",
    "    {\n",
    "        \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "result = pd.merge(left, right, on=\"key\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a more complicated example with multiple join keys. Only the keys appearing in left and right are present (the intersection), since **how='inner' by default**.\n",
    "\n",
    "![merge_key2](../images/pd_merging_merge_on_key_multiple.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame(\n",
    "    {\n",
    "        \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"],\n",
    "        \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n",
    "        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n",
    "        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "right = pd.DataFrame(\n",
    "    {\n",
    "        \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"],\n",
    "        \"key2\": [\"K0\", \"K0\", \"K0\", \"K0\"],\n",
    "        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "result = pd.merge(left, right, on=[\"key1\", \"key2\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **how** argument to merge specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be NA. Here is a summary of the how options and their SQL equivalent names:\n",
    "\n",
    "* left: Use keys from left frame only\n",
    "* right: Use keys from right frame only\n",
    "* outer: Use union of keys from both frames\n",
    "* inner: Use intersection of keys from both frames\n",
    "* cross: Create the cartesian product of rows of both frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "left:\n",
    "![pd_merging_merge_on_key_left.png](../images/pd_merging_merge_on_key_left.png)\n",
    "\n",
    "right:\n",
    "![pd_merging_merge_on_key_right.png](../images/pd_merging_merge_on_key_right.png)\n",
    "\n",
    "inner:\n",
    "![pd_merging_merge_on_key_inner.png](../images/pd_merging_merge_on_key_inner.png)\n",
    "\n",
    "outer:\n",
    "![pd_merging_merge_on_key_outer.png](../images/pd_merging_merge_on_key_outer.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how=\"left\", on=[\"key1\", \"key2\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how=\"right\", on=[\"key1\", \"key2\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how=\"inner\", on=[\"key1\", \"key2\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how=\"outer\", on=[\"key1\", \"key2\"])\n",
    "\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special attention to cross:\n",
    "\n",
    "cross:\n",
    "![pd_merging_merge_on_key_cross.png](../images/pd_merging_merge_cross.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, how=\"cross\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging and multiindex\n",
    "\n",
    "You can merge a mult-indexed Series and a DataFrame, if the names of the MultiIndex correspond to the columns from the DataFrame. Transform the Series to a DataFrame using Series.reset_index() before merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Let\": [\"A\", \"B\", \"C\"], \"Num\": [1, 2, 3]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(\n",
    "    [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"],\n",
    "    index=pd.MultiIndex.from_arrays(\n",
    "        [[\"A\", \"B\", \"C\"] * 2, [1, 2, 3, 4, 5, 6]], names=[\"Let\", \"Num\"]\n",
    "    ),\n",
    ")\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df, ser.reset_index(), on=[\"Let\", \"Num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example\n",
    "\n",
    "![merging_merge_on_key_dup.png](../images/pd_merging_merge_on_key_dup.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({\"A\": [1, 2], \"B\": [2, 2]})\n",
    "\n",
    "right = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [2, 2, 2]})\n",
    "\n",
    "result = pd.merge(left, right, on=\"B\", how=\"outer\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate\n",
    "\n",
    "Users can use the validate argument to automatically check whether there are unexpected duplicates in their merge keys. Key uniqueness is checked before merge operations and so should protect against memory overflows. Checking key uniqueness is also a good way to ensure user data structures are as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({\"A\": [1, 2], \"B\": [1, 2]})\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [2, 2, 2]})\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, on=\"B\", how=\"outer\", validate=\"one_to_one\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user is aware of the duplicates in the right DataFrame but wants to ensure there are no duplicates in the left DataFrame, one can use the validate='one_to_many'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=\"B\", how=\"outer\", validate=\"one_to_many\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge indicator\n",
    "\n",
    "merge() accepts the argument indicator. If True, a Categorical-type column called _merge will be added to the output object that takes on values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"col1\": [0, 1], \"col_left\": [\"a\", \"b\"]})\n",
    "\n",
    "df2 = pd.DataFrame({\"col1\": [1, 2, 2], \"col_right\": [2, 2, 2]})\n",
    "\n",
    "pd.merge(df1, df2, on=\"col1\", how=\"outer\", indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging ordered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A merge_ordered() function allows combining time series and other ordered data. In particular it has an optional fill_method keyword to fill/interpolate missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame(\n",
    "    {\"k\": [\"K0\", \"K1\", \"K1\", \"K2\"], \"lv\": [1, 2, 3, 4], \"s\": [\"a\", \"b\", \"c\", \"d\"]}\n",
    ")\n",
    "right = pd.DataFrame({\"k\": [\"K1\", \"K2\", \"K4\"], \"rv\": [1, 2, 3]})\n",
    "\n",
    "pd.merge_ordered(left, right, fill_method=\"ffill\", left_by=\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frequency conversion and resampling of time series\n",
    "\n",
    "The object must have a datetime-like index (DatetimeIndex, PeriodIndex, or TimedeltaIndex), or the caller must pass the label of a datetime-like series/index to the on/level keyword parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range('1/1/2000', periods=9, freq='T')\n",
    "series = pd.Series(range(9), index=index)\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.resample('3T').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.resample('30S').asfreq()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.resample('30S').ffill()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.resample('30S').bfill()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def custom_resampler(arraylike):\n",
    "    return np.sum(arraylike) + 5\n",
    "\n",
    "series.resample('3T').apply(custom_resampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply\n",
    "\n",
    "Apply a function along an axis of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(np.sum, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(np.sum, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda x: [1, 2], axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning a Series inside the function is similar to passing result_type='expand'.\n",
    "# The resulting column names will be the Series index.\n",
    "\n",
    "df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing result_type='broadcast' will ensure the same shape result, \n",
    "# whether list-like or scalar is returned by the function, \n",
    "# and broadcast it along the axis. \n",
    "# The resulting column names will be the originals.\n",
    "\n",
    "df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
